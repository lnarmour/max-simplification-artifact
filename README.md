# max-simplification-artifact

This repository contains code samples that complement the POPL 2025 conference paper titled, "Maximal Simplification of Polyhedral Reductions". The paper proposes extensions to the simplifying reductions algorithm, which automatically obtains asmpytotic complexity improvments on the input program. The included (A)lpha to (C) (C)ompiler, `acc`, script contains a partial proof-of-concept implementation of the splitting methods proposed in the paper. 

## References

Title: Maximal Simplification of Polyhedral Reductions

Authors: Louis Narmour, Tomofumi Yuki, Sanjay Rajopadhye

[Archived Artifact](https://doi.org/10.5281/zenodo.13943008)

[Artifact Submission Guidelines](https://github.com/mlcommons/ck/blob/master/docs/artifact-evaluation/submission.md)

## Artifact check-list

* **Compilation:** (A)lpha to (C) (C)ompiler provided as java JAR files with make and gcc
* **Binary:** To be produced on target machine
* **Execution:** Automated via provided command line script
* **Experiments:** Asymptotic complexity verification of generated codes
* **How much disk space required (approximately)?:** ~300 MB
* **How much memory required (approximately)?:** ~512 MB
* **How much time is needed to prepare workflow (approximately)?:** <20 min (to install system dependencies if necessary)
* **How much time is needed to complete experiments (approximately)?:** <1 hour (to generate codes and run execution scripts)
* **Publicly available?:** Yes
* **Code licenses (if publicly available)?:** MIT License

## Description

The `inputs` directory contains several polynomial time complexity input programs. This directory contains the following three scripts:
1. `acc`: takes an input Alpha program and generates simplified C implementations with improved asymptotic complexity.
1. `build.sh`: runs `acc` on the input programs, generates C codes for the baseline program, each of the discovered simplifications, and runs correctness checks.
1. `validate-complexity.sh`: runs the compiled programs generated by `build.sh` and reports asymtotic complexity plots to verify the claims below.

### Claims

All of the input programs provided here, except `inputs/example_4d.alpha`, admit a linear complexity, $O(N)$, representation discoverable by simplification, some of which require splitting as discussed in the paper. 

The 4-dimensional one, however, indeed has a $O(N^2)$ simplification but the the current partial implementation does not currently handle it. We provide a manual simplification of this which has the expected complexity using the techniques described in the paper.

To validate these claims, simply run the following commands:
```
./build.sh
./validate-complexity.sh
```
This should produce the following plots:
```
plot_increasing_max_filter.png
plot_prefix_max.png
plot_prefix_sum.png
```

## Software prerequisites

Running this requires a Linux or Mac machine with:
* java 11
* gcc
* make

To generate plots for simple validation, this also requires:
* python3 (matplotlib, numpy)

If needed, to install matplotlib and numpy, we recommend using a virtual environment. This can be done with the following commands.

Create a new virtual environment:
```
python3 -mvenv virtual-py
```

Activate the environment:
```
source virtual-py/bin/activate
```

Install the packages with pip (python's package manager):
```
pip install matplotlib numpy
```

## Code generation

The `acc` script can be use to generate C code for an input Alpha program and has the following usage:
```
usage:
  acc [-m] [-o out_dir] [-l | --face-lattice DOMAIN] [--i]
      [-s [--target-complexity] [--num-simplifications] [--try-splitting]]
      [ALPHA_FILE]
options:
    -o,  --out-dir             : Directory in which to place the output files, if the
                                 path does not exist then it will be created.
                                 (default: ./<system_name>/)
    -d,  --data-type           : Data type to use for program variables, may be 'int',
                                 'long', 'float', or 'double' (default: 'float')
    -m,  --make                : Run make to build the generated files. Note, this may
                                 fail if both v1 and v2 files have not been generated.
                                 Cannot be used with -s since multiple versions may be
                                 generated. (default: false)
    -s,  --simplify            : Run the simplifying reductions algorithm on v2 input
                                 (default: false)
         --num-simplifications : Stop after a number of simplifications is found, or the
                                 exploration terminates, whichever comes first. The value
                                 of 0 tries to find all simplifications. (default: 1)
         --target-complexity   : Target simplified complexity (default: one less than
                                 the input program's complexity)
         --try-splitting       : Consider splits during simplification (default: false)
    -l                         : Build and show the face lattice for all reductions in
                                 the input program. Ignored if --face-lattice specified.
         --face-lattice        : Build and show the face lattice for the DOMAIN. The
                                 value of DOMAIN should be an isl basic set string.
    -i,  --illustrate          : Illustrate working example from Section 3.3 of the paper.
    -v,  --verbose             : Emit debug information during simplification exploration
arguments:
    ALPHA_FILE                 : Input Alpha file used to generate main ystem, makefile,
                                 wrapper, and verification files.
```

For example, use the following command to simplify the prefix sum example and generate corresponding C codes:
```
./acc -s --target-complexity 1 --num-simplifications 0 inputs/prefix_sum.alpha
```
This creates a subdirectory called `prefix_sum` which can be compiled with the following command:
```
make -C prefix_sum
```
This produces the following files:
```
prefix_sum/
├── Makefile
├── prefix_sum-wrapper.c
├── prefix_sum.ab
├── prefix_sum.c                <-- Baseline (input) program implementation
├── prefix_sum_verify.c
└── simplifications
    ├── v0
    │   ├── prefix_sum.alpha
    │   └── prefix_sum.c        <-- Simplified version 0
    └── v1
        ├── prefix_sum.alpha
        └── prefix_sum.c        <-- Simplified version 1
```
The simplifications subdirectory contains each of the discovered simplifications. There are two here (Section 2.1). 

### Compiling the baseline implementation

The baseline implementation can be compiled simply by running make. For example, the following commands make and run the baseline implementation for a problem size of N=100000.
```
$ cp prefix_sum/simplifications/v0/prefix_sum.c prefix_sum/
$ make -C prefix_sum
$ ./prefix_sum/prefix_sum 100000
Execution time : 4.751179 sec.
```
(Note the `$` is part of the shell prompt in this instance and not the command)

### Compiling the simplified implementations

The simplified versions can be compiled by copying the corresponding `*.c` file to the top level directory and running make. For example, use the following to compile and run the first simplified version for the problem size of N=100000.
```
$ cp generated/prefix_sum/simplifications/v0/prefix_sum.c generated/prefix_sum/
$ make -C prefix_sum
$ ./prefix_sum/prefix_sum 100000
Execution time : 0.004037 sec.
```

The execution time is drastically lower, as expected.

### Build and compile all examples

The `build.sh` scripts automates everything described above. It compiles baseline, simplified, and verification binaries for each of the input programs. The verification binary passes the same input data arrays to the baseline and simplified code and asserts that the output is identical. This is used as a sanity check to verify that the simplified implementations are correct.
```
$ ./build.sh
```
All binaries are placed in a subdirectory called `generated/bin`
```
generated
└── bin
    ├── working_example_baseline
    ├── working_example_simplified_v0
    ├── working_example_simplified_v0.verify
    ├── working_example_simplified_v1
    ├── working_example_simplified_v1.verify
    ├── working_example_simplified_v2
    ├── working_example_simplified_v2.verify
    ├── working_example_simplified_v3
    ├── working_example_simplified_v3.verify
    ├── ...
```

## Supplementary Functionality

The following aspects of the tool may be used to complement the discussion in the paper.

### Face Lattice

Given a convex domain expressed as an [isl](https://libisl.sourceforge.io/manual.pdf) basic set, the `-l` and `--face-lattice` options can be used to build its corresponding face lattice. 

#### Given an input domain (ISLBasicSet)

Use the following the emit the face lattice of the M by N rectangular domain:
```
acc --face-lattice '[N,M]->{[i,j] : 0<=i<=N and 0<=j<=M}'
```
```
Lattice:
  2-faces: {}
  1-faces: {0}, {1}, {2}, {3}
  0-faces: {0,2}, {0,3}, {1,2}, {1,3}
Constraints:
  0: [N, M] -> { [i, j] : i >= 0 }
  1: [N, M] -> { [i, j] : N - i >= 0 }
  2: [N, M] -> { [i, j] : j >= 0 }
  3: [N, M] -> { [i, j] : M - j >= 0 }
Faces:
  {}: [N, M] -> { [i, j] : 0 <= i <= N and 0 <= j <= M }
  {0}: [N, M] -> { [i, j] : i = 0 and N >= 0 and 0 <= j <= M }
  {1}: [N, M] -> { [i, j] : i = N and N >= 0 and 0 <= j <= M }
  {2}: [N, M] -> { [i, j] : j = 0 and M >= 0 and 0 <= i <= N }
  {3}: [N, M] -> { [i, j] : j = M and M >= 0 and 0 <= i <= N }
  {0,2}: [N, M] -> { [i, j] : i = 0 and j = 0 and N >= 0 and M >= 0 }
  {0,3}: [N, M] -> { [i, j] : i = 0 and j = M and N >= 0 and M >= 0 }
  {1,2}: [N, M] -> { [i, j] : i = N and j = 0 and N >= 0 and M >= 0 }
  {1,3}: [N, M] -> { [i, j] : i = N and j = M and N >= 0 and M >= 0 }
```

As expected, it has four 1-faces (edges) and four 0-faces (vertices).

#### Given an input Alpha program with reductions

Use the following the emit the face lattice of the bodies of any reduction expressions appearing in the input program:
```
acc -l inputs/prefix_sum.alpha
```
```
Expression:
  reduce(+, (i,j->i), {: 0 <= j <= i } : X[j])
Lattice:
  2-faces: {}
  1-faces: {0}, {1}, {2}
  0-faces: {0,1}, {0,2}, {1,2}
Constraints:
  0: [N] -> { [i, j] : N - i >= 0 }
  1: [N] -> { [i, j] : j >= 0 }
  2: [N] -> { [i, j] : i - j >= 0 }
Faces:
  {}: [N] -> { [i, j] : N > 0 and i <= N and 0 <= j <= i }
  {0}: [N] -> { [i, j] : i = N and N > 0 and 0 <= j <= N }
  {1}: [N] -> { [i, j] : j = 0 and N > 0 and 0 <= i <= N }
  {2}: [N] -> { [i, j] : j = i and N > 0 and 0 <= i <= N }
  {0,1}: [N] -> { [i, j] : i = N and j = 0 and N > 0 }
  {0,2}: [N] -> { [i, j] : i = N and j = N and N > 0 }
  {1,2}: [N] -> { [i, j] : i = 0 and j = 0 and N > 0 }
```

## Illustration of the working example

The working example from Section 3.3 is provided as a sample program, `inputs/working_example.alpha`. As written, this program has $O(N^3)$ complexity. The `-i` option can be used to illustrate one application of single-step simplification to obtain an $O(N^2)$ intermediate representation. This complements the presentation of Section 3.
```
acc -i
```
It generates the following output:
```
# The reduction from Equation 8 is expressed as the following Alpha program:
#
affine working_example [N] -> {  : N > 0 }
	inputs
		X : {[k]: 0 <= k <= N }
	outputs
		Y : {[i]: 0 <= i <= N }
	when {  : N > 0 } let
		Y[i] = reduce(+, (i,j,k->i), {: i <= N and j >= 0 and 0 <= k <= i - j } : X[k]);
.


# The reduction body, D, is the following 3D domain (ISLSet):
#
[N] -> { [i, j, k] : N > 0 and i <= N and j >= 0 and 0 <= k <= i - j }


# The write function, fp, is the following (ISLMultiAff):
#
[N] -> { [i, j, k] -> [(i)] }


# The read function, fd, is the following (ISLMultiAff):
#
[N] -> { [i, j, k] -> [(k)] }


# The accumulation space, A, is the following 2D domain (null space of fp):
#
[N] -> { [i, j, k] : i = 0 }


# The reuse space, R, is the following 2D domain (null space of fd):
#
[N] -> { [i, j, k] : k = 0 }


# The face lattice (Section 3.2) of D is:
#
Lattice:
  3-faces: {}
  2-faces: {0}, {1}, {2}, {3}
  1-faces: {0,1}, {0,2}, {0,3}, {1,2}, {1,3}, {2,3}
  0-faces: {0,1,2}, {0,1,3}, {0,2,3}, {1,2,3}
Constraints:
  0: [N] -> { [i, j, k] : N - i >= 0 }
  1: [N] -> { [i, j, k] : j >= 0 }
  2: [N] -> { [i, j, k] : k >= 0 }
  3: [N] -> { [i, j, k] : i - j - k >= 0 }
Faces:
  {}: [N] -> { [i, j, k] : N > 0 and i <= N and j >= 0 and 0 <= k <= i - j }
  {0}: [N] -> { [i, j, k] : i = N and N > 0 and j >= 0 and 0 <= k <= N - j }
  {1}: [N] -> { [i, j, k] : j = 0 and N > 0 and i <= N and 0 <= k <= i }
  {2}: [N] -> { [i, j, k] : k = 0 and N > 0 and i <= N and 0 <= j <= i }
  {3}: [N] -> { [i, j, k] : k = i - j and N > 0 and i <= N and 0 <= j <= i }
  {0,1}: [N] -> { [i, j, k] : i = N and j = 0 and N > 0 and 0 <= k <= N }
  {0,2}: [N] -> { [i, j, k] : i = N and k = 0 and N > 0 and 0 <= j <= N }
  {0,3}: [N] -> { [i, j, k] : i = N and k = N - j and N > 0 and 0 <= j <= N }
  {1,2}: [N] -> { [i, j, k] : j = 0 and k = 0 and N > 0 and 0 <= i <= N }
  {1,3}: [N] -> { [i, j, k] : j = 0 and k = i and N > 0 and 0 <= i <= N }
  {2,3}: [N] -> { [i, j, k] : j = i and k = 0 and N > 0 and 0 <= i <= N }
  {0,1,2}: [N] -> { [i, j, k] : i = N and j = 0 and k = 0 and N > 0 }
  {0,1,3}: [N] -> { [i, j, k] : i = N and j = 0 and k = N and N > 0 }
  {0,2,3}: [N] -> { [i, j, k] : i = N and j = N and k = 0 and N > 0 }
  {1,2,3}: [N] -> { [i, j, k] : i = 0 and j = 0 and k = 0 and N > 0 }


# The four constraints are labeled 0-3. The 2-faces are denoted as {0}, {1},
# {2}, {3}. The number inside the curly braces denotes which of these constraints
# is saturated (i.e., an equality) on that face. The domain of the {0}-face is,
# 
#   [N] -> { [i, j, k] : i = N and N > 0 and j >= 0 and 0 <= k <= N - j }
# 
# because the "0" constraint,
# 
#   [N] -> { [i, j, k] : N - i >= 0 }
# 
# becomes "i=N" when saturated.
# 
# Each face is characterized by it's normal vector in the context of its parent.
# The normal vectors for all of the 2-faces are:
#
{0}: [-1, 0, 0]
{1}: [0, 1, 0]
{2}: [0, 0, 1]
{3}: [1, -1, -1]


# The single-step simplification (Section 3.4) involves selecting a reuse vector (rho)
# in R and applying Theorem 5 from GR06.
# 
# There are infintiely many such vectors in R. However, we only need to enumerate 
# the onesthat result in unique combinations of residual computations on the four
# 2-faces.
# 
# There are three types of residual computations, per Section 3.5:
#   * POS-faces
#   * NEG-faces
#   * ZERO-faces
# 
# This means there are 3^4 = 81 combinations of labels (i.e., labelings). Many of
# these are not possible. Candidate reuse vectors for the 12 possible ones (Figure 5)
# are shown below:
#
  {0},  {1},  {2},  {3}    [candidate reuse vector]
------------------------
[ POS, ZERO, ZERO, ZERO]
[ NEG, ZERO, ZERO, ZERO]
[ZERO,  POS, ZERO, ZERO]
[ POS,  POS, ZERO, ZERO]
[ NEG,  POS, ZERO, ZERO] : [1, 1, 0]
[ZERO,  NEG, ZERO, ZERO]
[ POS,  NEG, ZERO, ZERO] : [-1, -1, 0]
[ NEG,  NEG, ZERO, ZERO]
[ZERO, ZERO,  POS, ZERO]
[ POS, ZERO,  POS, ZERO]
[ NEG, ZERO,  POS, ZERO]
[ZERO,  POS,  POS, ZERO]
[ POS,  POS,  POS, ZERO]
[ NEG,  POS,  POS, ZERO]
[ZERO,  NEG,  POS, ZERO]
[ POS,  NEG,  POS, ZERO]
[ NEG,  NEG,  POS, ZERO]
[ZERO, ZERO,  NEG, ZERO]
[ POS, ZERO,  NEG, ZERO]
[ NEG, ZERO,  NEG, ZERO]
[ZERO,  POS,  NEG, ZERO]
[ POS,  POS,  NEG, ZERO]
[ NEG,  POS,  NEG, ZERO]
[ZERO,  NEG,  NEG, ZERO]
[ POS,  NEG,  NEG, ZERO]
[ NEG,  NEG,  NEG, ZERO]
[ZERO, ZERO, ZERO,  POS]
[ POS, ZERO, ZERO,  POS]
[ NEG, ZERO, ZERO,  POS] : [1, 0, 0]
[ZERO,  POS, ZERO,  POS]
[ POS,  POS, ZERO,  POS]
[ NEG,  POS, ZERO,  POS] : [2, 1, 0]
[ZERO,  NEG, ZERO,  POS] : [0, -1, 0]
[ POS,  NEG, ZERO,  POS] : [-1, -2, 0]
[ NEG,  NEG, ZERO,  POS] : [1, -1, 0]
[ZERO, ZERO,  POS,  POS]
[ POS, ZERO,  POS,  POS]
[ NEG, ZERO,  POS,  POS]
[ZERO,  POS,  POS,  POS]
[ POS,  POS,  POS,  POS]
[ NEG,  POS,  POS,  POS]
[ZERO,  NEG,  POS,  POS]
[ POS,  NEG,  POS,  POS]
[ NEG,  NEG,  POS,  POS]
[ZERO, ZERO,  NEG,  POS]
[ POS, ZERO,  NEG,  POS]
[ NEG, ZERO,  NEG,  POS]
[ZERO,  POS,  NEG,  POS]
[ POS,  POS,  NEG,  POS]
[ NEG,  POS,  NEG,  POS]
[ZERO,  NEG,  NEG,  POS]
[ POS,  NEG,  NEG,  POS]
[ NEG,  NEG,  NEG,  POS]
[ZERO, ZERO, ZERO,  NEG]
[ POS, ZERO, ZERO,  NEG] : [-1, 0, 0]
[ NEG, ZERO, ZERO,  NEG]
[ZERO,  POS, ZERO,  NEG] : [0, 1, 0]
[ POS,  POS, ZERO,  NEG] : [-1, 1, 0]
[ NEG,  POS, ZERO,  NEG] : [1, 2, 0]
[ZERO,  NEG, ZERO,  NEG]
[ POS,  NEG, ZERO,  NEG] : [-2, -1, 0]
[ NEG,  NEG, ZERO,  NEG]
[ZERO, ZERO,  POS,  NEG]
[ POS, ZERO,  POS,  NEG]
[ NEG, ZERO,  POS,  NEG]
[ZERO,  POS,  POS,  NEG]
[ POS,  POS,  POS,  NEG]
[ NEG,  POS,  POS,  NEG]
[ZERO,  NEG,  POS,  NEG]
[ POS,  NEG,  POS,  NEG]
[ NEG,  NEG,  POS,  NEG]
[ZERO, ZERO,  NEG,  NEG]
[ POS, ZERO,  NEG,  NEG]
[ NEG, ZERO,  NEG,  NEG]
[ZERO,  POS,  NEG,  NEG]
[ POS,  POS,  NEG,  NEG]
[ NEG,  POS,  NEG,  NEG]
[ZERO,  NEG,  NEG,  NEG]
[ POS,  NEG,  NEG,  NEG]
[ NEG,  NEG,  NEG,  NEG]

# Lets look at the candidate choice shown in Figure 4 (note the numbering in Figure 4 
# starts from 1, not 0):
# 
#     {0},  {1},  {2},  {3}
#   [ NEG, ZERO, ZERO,  POS] : [1, 0, 0]
# 
# The {0} face, at i=N, is called a NEG-face by [1, 0, 0] because this candidate 
# choice of reuse has a negative dot product with its normal vector ([-1, 0, 0]).
# 
#   dot([1,0,0], [-1,0,0]) = 1*-1 + 0*0 + 0*0 = -1 < 0
# 
# Appliying the single-step simplication of GR06 with the reuse vector [1, 0, 0] gives
# the following simplified program:
#
affine working_example [N] -> {  : N > 0 }
	inputs
		X : {[k]: 0 <= k <= N }
	outputs
		Y : {[i]: 0 <= i <= N }
	locals
		Y_pos : {[i0]: 0 <= i0 <= N }
	when {  : N > 0 } let
		Y[i] = case  {
			{: i = 0 } : Y_pos;
			{: 0 < i <= N } : (Y_pos + Y[i-1]);
		};
		
		Y_pos[i0] = reduce(+, (i,j,k->i), {: k = i - j and i <= N and 0 <= j <= i } : X[k]);
.


# The simplified program now involves a residual 2D reduction "on" the {3}-face. The
# expression inside the reduction body is still X[k], but the surrounding context has the
# constraint that k=i-j. This corresponds precisely to the fact that the computation
# "moved" to the {3}-face which is characterized by saturing i - j - k >= 0.
# 
# Simplification (without splitting) fails when all labelings do not yield at least 
# one possible labeling.
#
```

The transformed program can be fed as input to the tool to generate quadratic complexity C code as well, if desired.

## Notes

The `acc` instance provided here is a snapshot of the corresponding GitHub repository (at the time of writing):  
[https://github.com/csu-cs-melange/alpha-codegen](https://github.com/csu-cs-melange/alpha-codegen)

Additional information about the concrete implementation can be found in this and its related repositories.
